{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a68af6d-e0b6-498b-9cc7-5601835be5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27b20c8-f0a9-48d5-a8de-c134e5bf8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_array(video, info):\n",
    "    all_frames = []\n",
    "    cap = cv2.VideoCapture(os.path.join('videos',video))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    dimensions = (int(cap.get(3)),int(cap.get(4)))\n",
    "    for frame_idx in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n",
    "        ret, frame = cap.read()\n",
    "        \"\"\"try:\n",
    "            frame = cv2.resize(frame, (200,200))\n",
    "        except:\n",
    "            break\"\"\"\n",
    "        all_frames.append(frame)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Close down everything\n",
    "    if info:\n",
    "        dimensions = (int(cap.get(3)),int(cap.get(4)))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return all_frames, fps, dimensions\n",
    "    else:\n",
    "        return all_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51b8b15-5b38-4e04-afc2-4f0e325504ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workout\n",
    "# play 10 second timer\n",
    "# play workout video\n",
    "# follow along\n",
    "# show similarity score in a popup next to it\n",
    "# save similarity score and live video\n",
    "# point out mistakes and differences\n",
    "\n",
    "def workout(instruct_vid): #, output_name\n",
    "    # cap1 = cv2.VideoCapture(os.path.join('videos','10secCountdown.mp4'))\n",
    "    # cap2 = cv2.VideoCapture(os.path.join('videos','jumpingjacks.mp4'))\n",
    "    all_frames_vid1 = frame_array('10secCountdown.mp4', False)\n",
    "    all_frames_vid2, fps2, dimensions2 = frame_array(instruct_vid, True)\n",
    "    \n",
    "    # show 10 sec countdown\n",
    "    for frame1 in all_frames_vid1:\n",
    "        cv2.imshow(\"Video Player\", frame1)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # show live video at same time as workout video\n",
    "    cap_live = cv2.VideoCapture(0)\n",
    "    fps_live = cap_live.get(cv2.CAP_PROP_FPS)\n",
    "    dimensions_live = (int(cap_live.get(3)),int(cap_live.get(4)))\n",
    "    \"\"\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap_live.get(cv2.CAP_PROP_FPS)\n",
    "    video_writer = cv2.VideoWriter(os.path.join('videos','outputs',output_name), \n",
    "                                   fourcc, fps, (int(cap_live.get(3)),int(cap_live.get(4)))) \"\"\"\n",
    "    live_frames = [0] * len(all_frames_vid2)\n",
    "    for frame_idx, frame2 in enumerate(all_frames_vid2):\n",
    "        cv2.imshow('Video Player', frame2)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "        ret_live, frame_live = cap_live.read()\n",
    "        live_frames[frame_idx] = frame_live\n",
    "        # video_writer.write(frame_live)    \n",
    "    \n",
    "    cap_live.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return all_frames_vid2, live_frames, fps2, fps_live, dimensions2, dimensions_live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca77a0a-0f36-4fd6-bed0-a9b844436bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [], 0.0, 1.0, (0, 0), (1280, 720))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workout(\"cutjumpjacks360.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f53d6b89-6bbc-44c1-b2c6-efa106911058",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='lite-model_movenet_singlepose_lightning_3.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a07276e-a2ef-473a-a79d-9d9c14080dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(frame, keypoints, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y, x, 1])) # confidence metric c, dont transform that\n",
    "    \n",
    "    for kp in shaped:\n",
    "        ky, kx, kp_conf = kp\n",
    "        if kp_conf > confidence_threshold:\n",
    "            cv2.circle(frame, (int(kx), int(ky)), 4, (0,255,0), -1) #.circle creates circle, 4 is size, 255 is color green, -1 is fill circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba514d5-fd42-4107-994c-3cfdd7f837ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nose, left eye, right eye, left ear, right ear, left shoulder, right shoulder, left elbow, right elbow, \n",
    "# left wrist, right wrist, left hip, right hip, left knee, right knee, left ankle, right ankle\n",
    "\n",
    "EDGES = {\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "934847e8-26d7-4603-a217-a0ed53824188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_connections(frame, keypoints, edges, confidence_threshold):\n",
    "    y, x, c = frame.shape\n",
    "    shaped = np.squeeze(np.multiply(keypoints, [y,x,1]))\n",
    "    \n",
    "    for edge, color in edges.items():\n",
    "        p1, p2 = edge\n",
    "        y1, x1, c1 = shaped[p1]\n",
    "        y2, x2, c2 = shaped[p2]\n",
    "        \n",
    "        if (c1 > confidence_threshold) & (c2 > confidence_threshold):      \n",
    "            cv2.line(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93810398-5c9f-4f64-925b-ab4596e96fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    # a is first angle, b is second angle, c is 3rd angle\n",
    "    \n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0/np.pi)\n",
    "    \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "993d7187-d4fc-4764-a06a-18455c4b4d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_comparison(all_frames1, all_frames2, fps1, fps2, output1, output2, dimensions1, dimensions2):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer1 = cv2.VideoWriter(os.path.join('videos','outputs',output1), \n",
    "                                   fourcc, fps1, dimensions1)\n",
    "    video_writer2 = cv2.VideoWriter(os.path.join('videos','outputs',output2), \n",
    "                                   fourcc, fps2, dimensions2)\n",
    "    for frame_idx, frame1 in enumerated(all_frames1):\n",
    "        frame2 = all_frames2[frame_idx]\n",
    "        if (frame1 is None) or (frame2 is None):\n",
    "            break\n",
    "        img1 = frame1.copy()\n",
    "        img1 = tf.image.resize_with_pad(tf.expand_dims(img1, axis = 0), 192, 192)\n",
    "        input_image1 = tf.cast(img1, dtype=tf.float32)\n",
    "        \n",
    "        img2 = frame2.copy()\n",
    "        img2 = tf.image.resize_with_pad(tf.expand_dims(img2, axis = 0), 192, 192)\n",
    "        input_image2 = tf.cast(img2, dtype=tf.float32)\n",
    "\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image1))\n",
    "        interpreter.invoke()\n",
    "        keypoints_with_scores1 = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        wanted_scores1 = keypoints_with_scores1[0][0][5:]\n",
    "        draw_connections(frame1, keypoints_with_scores1, EDGES, 0.6)\n",
    "        draw_keypoints(frame1, wanted_scores1, 0.6)\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image2))\n",
    "        interpreter.invoke()\n",
    "        keypoints_with_scores2 = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "        wanted_scores2 = keypoints_with_scores2[0][0][5:]\n",
    "        draw_connections(frame2, keypoints_with_scores2, EDGES, 0.6)\n",
    "        draw_keypoints(frame2, wanted_scores2, 0.6)\n",
    "\n",
    "        video_writer1.write(frame1) \n",
    "        video_writer2.write(frame2)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    return wanted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1243d4b-0524-4124-a5c5-2667d7bbad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_process():\n",
    "    vid_frames, live_frames, fps_vid, fps_live = workout('cutjumpjacks360.mp4')\n",
    "    scores_vid = add_keypoints_frames(vid_frames, \"instructWkeypoints.mp4\", fps_vid)\n",
    "    scores_live = add_keypoints_frames(vid_frames, \"instructWkeypoints.mp4\", fps_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71843f48-602d-421d-aa02-e91c63edecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horiz_merge_two_videos(video1, video2):\n",
    "    cap1 = cv2.VideoCapture(os.path.join('videos',video1))\n",
    "    cap2 = cv2.VideoCapture(os.path.join('videos',video2))\n",
    "    \n",
    "    frame_width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_rate = int(cap1.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('output.mp4', fourcc, frame_rate, (frame_width * 2, frame_height))\n",
    "    \n",
    "    while True:\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "        frame2 = cv2.resize(frame2, (frame_width, frame_height))\n",
    "        canvas = np.zeros((frame_height, frame_width * 2, 3), dtype = np.uint8)\n",
    "        canvas[:, :frame_width] = frame1\n",
    "        canvas[:, frame_width:] = frame2\n",
    "        out.write(canvas)\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7316a8b7-d0e1-428a-834f-49027830d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vert_merge_two_videos(video1, video2, output_name):\n",
    "    cap1 = cv2.VideoCapture(os.path.join('videos',video1))\n",
    "    cap2 = cv2.VideoCapture(os.path.join('videos',video2))\n",
    "    \n",
    "    frame_width = int(cap1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap1.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    full_frame_height = int(frame_height * 1.5)\n",
    "    bottom_frame_height = full_frame_height - frame_height\n",
    "    out = cv2.VideoWriter(output_name, fourcc, fps, (frame_width, full_frame_height))\n",
    "    \n",
    "    while True:\n",
    "        ret1, frame1 = cap1.read()\n",
    "        ret2, frame2 = cap2.read()\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "        frame2 = cv2.resize(frame2, (frame_width, bottom_frame_height))\n",
    "        canvas = np.zeros((full_frame_height, frame_width, 3), dtype = np.uint8)\n",
    "        canvas[:frame_height, :] = frame1\n",
    "        canvas[frame_height:, :] = frame2\n",
    "        out.write(canvas)\n",
    "    cap1.release()\n",
    "    cap2.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48a9dc60-5774-4c8c-8503-9fcfc6616752",
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_merge_two_videos(\"cutjumpjacks360.mp4\", 'jumpingjacks.mp4', 'output.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbadc5e-86e7-4581-af59-4d3c847e2f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
